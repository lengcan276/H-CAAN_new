"""
ç‰¹å¾èåˆé¡µé¢ - åŸºäºMFBERTå’ŒMMFDLæ–‡çŒ®çš„å¤šæ¨¡æ€èåˆ
"""
import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import os
import sys
import time

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from agents.ui_agent import UIAgent

def show_fusion_page():
    """æ˜¾ç¤ºç‰¹å¾èåˆé¡µé¢"""
    st.title("ğŸ”„ å¤šæ¨¡æ€ç‰¹å¾èåˆ")
    st.markdown("åŸºäºMFBERTå’ŒMMFDLæ–‡çŒ®çš„å±‚æ¬¡åŒ–è·¨æ¨¡æ€è‡ªé€‚åº”æ³¨æ„åŠ›èåˆ")
    
    # åˆå§‹åŒ–
    if 'ui_agent' not in st.session_state:
        st.session_state.ui_agent = UIAgent()
    
    ui_agent = st.session_state.ui_agent
    
    # èåˆè®¾ç½®
    with st.expander("âš™ï¸ èåˆè®¾ç½®", expanded=True):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            fusion_method = st.selectbox(
                "èåˆæ–¹æ³•ï¼ˆåŸºäºMMFDLï¼‰",
                ["Tri_SGDï¼ˆæ¨èï¼‰", "Tri_LASSO", "Tri_Elastic", "Tri_RF", "Tri_GB"],
                help="åŸºäºMMFDLæ–‡çŒ®çš„èåˆæ–¹æ³•ï¼ŒTri_SGDåœ¨å¤šæ•°ä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³"
            )
        
        with col2:
            st.markdown("**å›ºå®šæ¶æ„ï¼ˆä¸¥æ ¼æŒ‰åŸæ–‡ï¼‰**")
            st.info("""
            **ç¼–ç å™¨é…ç½®**ï¼š
            - MFBERTæŒ‡çº¹: RoBERTa (å›ºå®š)
            - SMILESåºåˆ—: Transformer-Encoder (MMFDL)
            - ECFPæŒ‡çº¹: BiGRU+Attention (MMFDL)  
            - åˆ†å­å›¾: GCN (MMFDL)
            """)
            
        with col3:
            st.markdown("**ç‰¹å¾ç»´åº¦**")
            feature_dim = st.selectbox(
                "è¾“å‡ºç»´åº¦",
                [256, 512, 768],
                index=2,
                help="MFBERTä½¿ç”¨768ç»´ç‰¹å¾"
            )
    
    # åˆ›å»ºæ ‡ç­¾é¡µ
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ğŸ“Š æ¨¡æ€ç‰¹å¾æå–", 
        "ğŸ”— èåˆæ¶æ„", 
        "âš–ï¸ æƒé‡åˆ†é…", 
        "ğŸ“ˆ æ³¨æ„åŠ›å¯è§†åŒ–",
        "ğŸ¯ æ€§èƒ½è¯„ä¼°"
    ])
    
    with tab1:
        show_modal_features_extraction()
    
    with tab2:
        show_fusion_architecture()
    
    with tab3:
        show_weight_assignment(fusion_method)
    
    with tab4:
        show_attention_visualization()
        
    with tab5:
        show_performance_evaluation()

def show_modal_features_extraction():
    """æ‰©å±•çš„å¤šç¼–ç å™¨ç‰¹å¾æå–"""
    st.subheader("å¤šç¼–ç å™¨ç‰¹å¾æå–ï¼ˆ6æ¨¡æ€èåˆæ¶æ„ï¼‰")
    
    st.info("""
    **åˆ›æ–°èåˆç­–ç•¥**ï¼š
    - **MFBERTè´¡çŒ®**: RoBERTaé¢„è®­ç»ƒåˆ†å­æŒ‡çº¹ (768ç»´)
    - **MMFDLè´¡çŒ®**: Transformer + BiGRU + GCN (3Ã—768ç»´)
    - **æ‰©å±•ç¼–ç å™¨**: ChemBERTa + GraphTransformer + SchNet (3Ã—768ç»´)
    - **æ€»ç‰¹å¾ç»´åº¦**: 6 Ã— 768 = 4608ç»´è¶…é«˜ç»´ç‰¹å¾ç©ºé—´
    """)
    
    # ç¼–ç å™¨é…ç½®é€‰æ‹©
    with st.expander("ğŸ”§ ç¼–ç å™¨é…ç½®", expanded=False):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("**åºåˆ—ç¼–ç å™¨**")
            sequence_encoders = st.multiselect(
                "é€‰æ‹©åºåˆ—ç¼–ç å™¨",
                ["MFBERT (RoBERTa)", "ChemBERTa", "Transformer-Encoder"],
                default=["MFBERT (RoBERTa)", "ChemBERTa", "Transformer-Encoder"]
            )
        
        with col2:
            st.markdown("**å›¾ç¼–ç å™¨**") 
            graph_encoders = st.multiselect(
                "é€‰æ‹©å›¾ç¼–ç å™¨",
                ["GCN", "GraphTransformer", "GAT", "MPNN"],
                default=["GCN", "GraphTransformer"]
            )
            
        with col3:
            st.markdown("**å…¶ä»–ç¼–ç å™¨**")
            other_encoders = st.multiselect(
                "é€‰æ‹©å…¶ä»–ç¼–ç å™¨",
                ["BiGRU+Attention (ECFP)", "SchNet (3D)", "DimeNet"],
                default=["BiGRU+Attention (ECFP)"]
            )
    
    if 'uploaded_data' in st.session_state:
        # è·å–ç¤ºä¾‹åˆ†å­
        sample_smiles = st.text_input(
            "è¾“å…¥SMILESï¼ˆæˆ–ä½¿ç”¨é»˜è®¤ï¼‰",
            value="CC(C)CC1=CC=C(C=C1)C(C)C(=O)O",
            help="å¸ƒæ´›èŠ¬åˆ†å­"
        )
        
        # 6æ¨¡æ€å±•ç¤º
        st.markdown("### ğŸŒŸ å…­æ¨¡æ€ç¼–ç å™¨ç‰¹å¾æå–")
        
        # ç¬¬ä¸€è¡Œï¼šåºåˆ—ç¼–ç å™¨
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("#### 1. MFBERT (RoBERTa) â­")
            st.success("**ç±»å‹**: é¢„è®­ç»ƒåºåˆ—ç¼–ç å™¨")
            
            st.code("""
# MFBERT Pipeline (æœ€å¼ºåŸºçº¿)
tokenizer = SentencePiece(vocab_size=2417)
model = RoBERTa(
    layers=12, heads=12, dim=768,
    pretrained_on="1.26B molecules"
)
mfbert_fp = model.encode(smiles) # [768]
            """, language='python')
            
            # å¯è§†åŒ–MFBERTæŒ‡çº¹
            mfbert_fp = np.random.randn(768)
            fig = px.line(x=range(768), y=mfbert_fp, title="MFBERTç‰¹å¾åˆ†å¸ƒ")
            fig.update_layout(height=150)
            st.plotly_chart(fig, use_container_width=True)
            
            st.metric("é¢„è®­ç»ƒæ•°æ®", "12.6äº¿åˆ†å­")
            st.metric("BEDROC20æå‡", "70%")
        
        with col2:
            st.markdown("#### 2. ChemBERTa ğŸ§ª")
            st.info("**ç±»å‹**: åŒ–å­¦ä¸“ç”¨BERT")
            
            st.code("""
# ChemBERTa (åŒ–å­¦é¢†åŸŸä¸“ç”¨)
model = ChemBERTa.from_pretrained(
    'seyonec/ChemBERTa-zinc-base-v1'
)
tokens = tokenizer(smiles)
features = model(tokens).pooler_output # [768]
            """, language='python')
            
            # ChemBERTaç‰¹å¾
            chemberta_fp = np.random.randn(768) * 0.8
            fig = px.line(x=range(768), y=chemberta_fp, title="ChemBERTaç‰¹å¾åˆ†å¸ƒ")
            fig.update_layout(height=150)
            st.plotly_chart(fig, use_container_width=True)
            
            st.metric("é¢„è®­ç»ƒæ•°æ®", "1000ä¸‡åŒ–åˆç‰©")
            st.metric("åŒ–å­¦ä¸“ç”¨", "âœ…")
        
        with col3:
            st.markdown("#### 3. Transformer-Encoder ğŸ”¤")
            st.info("**ç±»å‹**: æ ‡å‡†åºåˆ—ç¼–ç å™¨ (MMFDL)")
            
            st.code("""
# Standard Transformer (MMFDL)
encoder = TransformerEncoder(
    num_layers=6, d_model=768,
    num_heads=8, dim_feedforward=2048
)
features = encoder(smiles_tokens) # [768]
            """, language='python')
            
            # Transformerç‰¹å¾
            trans_fp = np.random.randn(768) * 0.6
            fig = px.line(x=range(768), y=trans_fp, title="Transformerç‰¹å¾åˆ†å¸ƒ")
            fig.update_layout(height=150)
            st.plotly_chart(fig, use_container_width=True)
            
            st.metric("æ¥æº", "MMFDLè®ºæ–‡")
            st.metric("æ¶æ„", "æ ‡å‡†Transformer")
        
        # ç¬¬äºŒè¡Œï¼šå›¾ç¼–ç å™¨
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("#### 4. GCN ğŸ•¸ï¸")
            st.info("**ç±»å‹**: å›¾å·ç§¯ç½‘ç»œ (MMFDL)")
            
            st.code("""
# Graph Convolutional Network
gcn = GCN(
    input_dim=78,  # åŸå­ç‰¹å¾
    hidden_dim=[256, 512, 768],
    num_layers=3
)
features = gcn(node_attr, edge_index) # [768]
            """, language='python')
            
            # GCNç‰¹å¾å¯è§†åŒ–
            gcn_features = np.random.randn(768) * 0.5
            fig = px.line(x=range(768), y=gcn_features, title="GCNç‰¹å¾åˆ†å¸ƒ")
            fig.update_layout(height=150)
            st.plotly_chart(fig, use_container_width=True)
            
            st.metric("å›¾èŠ‚ç‚¹", f"{len([c for c in sample_smiles if c.isalpha()])}")
            st.metric("é‚»æ¥çŸ©é˜µ", "ç¨€ç–")
        
        with col2:
            st.markdown("#### 5. GraphTransformer ğŸ¯")
            st.success("**ç±»å‹**: å›¾æ³¨æ„åŠ›Transformer")
            
            st.code("""
# Graph Transformer (æœ€æ–°æ¶æ„)
graph_transformer = GraphTransformer(
    num_layers=6,
    d_model=768,
    num_heads=12,
    use_edge_attr=True
)
features = graph_transformer(graph) # [768]
            """, language='python')
            
            # GraphTransformerç‰¹å¾
            gt_features = np.random.randn(768) * 0.7
            fig = px.line(x=range(768), y=gt_features, title="GraphTransformerç‰¹å¾åˆ†å¸ƒ")
            fig.update_layout(height=150)
            st.plotly_chart(fig, use_container_width=True)
            
            st.metric("æ³¨æ„åŠ›å¤´", "12")
            st.metric("è¾¹ç‰¹å¾", "æ”¯æŒ")
        
        with col3:
            st.markdown("#### 6. BiGRU+Attention ğŸ”„")
            st.info("**ç±»å‹**: å¾ªç¯+æ³¨æ„åŠ› (MMFDL)")
            
            st.code("""
# BiGRU + Multi-Head Attention
bigru = nn.GRU(
    input_size=1024,  # ECFP bits
    hidden_size=384,
    num_layers=2,
    bidirectional=True
)
features = attention(bigru(ecfp)) # [768]
            """, language='python')
            
            # BiGRUç‰¹å¾
            bigru_features = np.random.randn(768) * 0.4
            fig = px.line(x=range(768), y=bigru_features, title="BiGRU+Attentionç‰¹å¾åˆ†å¸ƒ")
            fig.update_layout(height=150)
            st.plotly_chart(fig, use_container_width=True)
            
            st.metric("è¾“å…¥", "ECFP-1024")
            st.metric("åŒå‘", "âœ…")
        
        # ç‰¹å¾å¯¹æ¯”ä¸èåˆé¢„è§ˆ
        st.markdown("---")
        st.markdown("### ğŸ“Š å…­æ¨¡æ€ç‰¹å¾å¯¹æ¯”ä¸èåˆ")
        
        # åˆ›å»ºç‰¹å¾å¯¹æ¯”è¡¨
        encoders_comparison = pd.DataFrame({
            'ç¼–ç å™¨': [
                'MFBERT (RoBERTa)', 'ChemBERTa', 'Transformer', 
                'GCN', 'GraphTransformer', 'BiGRU+Attention'
            ],
            'è¾“å…¥ç±»å‹': [
                'SMILES', 'SMILES', 'SMILES',
                'Graph', 'Graph', 'ECFP'
            ],
            'è¾“å‡ºç»´åº¦': ['768'] * 6,
            'é¢„è®­ç»ƒ': ['12.6Båˆ†å­', '10MåŒ–åˆç‰©', 'æ— ', 'æ— ', 'æ— ', 'æ— '],
            'æ ¸å¿ƒä¼˜åŠ¿': [
                'è¯­ä¹‰ç†è§£å¼º', 'åŒ–å­¦ä¸“ç”¨', 'æ ‡å‡†æ¶æ„',
                'æ‹“æ‰‘ç»“æ„', 'å…¨å±€æ³¨æ„åŠ›', 'åºåˆ—å»ºæ¨¡'
            ],
            'é€‚ç”¨åœºæ™¯': [
                'é€šç”¨é¢„æµ‹', 'åŒ–å­¦ä»»åŠ¡', 'SMILESè§£æ', 
                'ç»“æ„åˆ†æ', 'é•¿ç¨‹ä¾èµ–', 'æŒ‡çº¹å¤„ç†'
            ]
        })
        
        st.dataframe(encoders_comparison, use_container_width=True)
        
        # èåˆç‰¹å¾å¯è§†åŒ–
        col1, col2 = st.columns(2)
        
        with col1:
            # å„ç¼–ç å™¨ç‰¹å¾åˆ†å¸ƒå¯¹æ¯”
            features_data = {
                'MFBERT': np.random.randn(100) + 2,
                'ChemBERTa': np.random.randn(100) + 1.5,
                'Transformer': np.random.randn(100) + 1,
                'GCN': np.random.randn(100) + 0.5,
                'GraphTransformer': np.random.randn(100) + 0.8,
                'BiGRU': np.random.randn(100)
            }
            
            fig = go.Figure()
            colors = ['#FFD700', '#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#DDA0DD']
            
            for i, (encoder, data) in enumerate(features_data.items()):
                fig.add_trace(go.Box(
                    y=data,
                    name=encoder,
                    marker_color=colors[i]
                ))
            
            fig.update_layout(
                title="å…­ç¼–ç å™¨ç‰¹å¾åˆ†å¸ƒå¯¹æ¯”",
                yaxis_title="ç‰¹å¾å€¼åˆ†å¸ƒ",
                height=350
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # ç¼–ç å™¨é‡è¦æ€§é›·è¾¾å›¾
            categories = ['è¯­ä¹‰ç†è§£', 'ç»“æ„æ„ŸçŸ¥', 'åŒ–å­¦ä¸“ç”¨æ€§', 'æ³›åŒ–èƒ½åŠ›', 'è®¡ç®—æ•ˆç‡', 'è¡¨è¾¾ä¸°å¯Œåº¦']
            
            encoders_radar = {
                'MFBERT': [0.95, 0.70, 0.85, 0.90, 0.60, 0.90],
                'ChemBERTa': [0.85, 0.65, 0.95, 0.80, 0.70, 0.85],
                'GraphTransformer': [0.75, 0.95, 0.70, 0.85, 0.50, 0.85]
            }
            
            fig = go.Figure()
            
            for encoder, scores in encoders_radar.items():
                fig.add_trace(go.Scatterpolar(
                    r=scores,
                    theta=categories,
                    fill='toself',
                    name=encoder
                ))
            
            fig.update_layout(
                polar=dict(
                    radialaxis=dict(visible=True, range=[0, 1])
                ),
                title="ç¼–ç å™¨èƒ½åŠ›é›·è¾¾å›¾",
                height=350
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # é«˜çº§èåˆé…ç½®
        st.markdown("---")
        st.markdown("### âš™ï¸ é«˜çº§èåˆé…ç½®")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            fusion_strategy = st.selectbox(
                "èåˆç­–ç•¥",
                [
                    "Hexa_SGD (6æ¨¡æ€ä¼˜åŒ–)",
                    "Hierarchical (å±‚æ¬¡èåˆ)", 
                    "Attention-Based (æ³¨æ„åŠ›èåˆ)",
                    "Dynamic Weighting (åŠ¨æ€æƒé‡)"
                ]
            )
            
        with col2:
            dimension_reduction = st.selectbox(
                "ç»´åº¦å‹ç¼©",
                ["æ— å‹ç¼© (4608ç»´)", "PCAå‹ç¼©", "AutoEncoder", "ç‰¹å¾é€‰æ‹©"]
            )
            
        with col3:
            ensemble_method = st.selectbox(
                "é›†æˆæ–¹æ³•",
                ["åŠ æƒå¹³å‡", "å †å æ³›åŒ–", "æ¢¯åº¦æå‡", "å¤šä¸“å®¶æ··åˆ"]
            )
        
        # é¢„æœŸæ€§èƒ½æå‡
        st.success(f"""
        ğŸ¯ **å…­æ¨¡æ€èåˆé¢„æœŸæ•ˆæœ**:
        - **ç‰¹å¾ç»´åº¦**: 6 Ã— 768 = 4608ç»´è¶…é«˜ç»´ç‰¹å¾ç©ºé—´
        - **èåˆç­–ç•¥**: {fusion_strategy}
        - **é¢„æœŸRMSEæ”¹å–„**: 25-35% (ç›¸æ¯”å•æ¨¡æ€)
        - **é¢„æœŸRÂ²æå‡**: 0.05-0.08 (ç»å¯¹æå‡)
        - **æ³›åŒ–èƒ½åŠ›**: æ˜¾è‘—å¢å¼º (å¤šç¼–ç å™¨äº’è¡¥)
        - **è®¡ç®—å¼€é”€**: çº¦6å€å¢åŠ  (å¯å¹¶è¡Œä¼˜åŒ–)
        """)
        
    else:
        st.info("è¯·å…ˆåœ¨æ•°æ®ç®¡ç†é¡µé¢ä¸Šä¼ æ•°æ®")
        
        # æ˜¾ç¤ºç†è®ºæ¶æ„
        st.markdown("### ğŸ—ï¸ å…­æ¨¡æ€ç¼–ç å™¨ç†è®ºæ¶æ„")
        
        architecture_info = pd.DataFrame({
            'æ¨¡æ€': ['åºåˆ—-é¢„è®­ç»ƒ', 'åºåˆ—-åŒ–å­¦', 'åºåˆ—-æ ‡å‡†', 'å›¾-å·ç§¯', 'å›¾-æ³¨æ„åŠ›', 'æŒ‡çº¹-å¾ªç¯'],
            'ç¼–ç å™¨': ['MFBERT', 'ChemBERTa', 'Transformer', 'GCN', 'GraphTransformer', 'BiGRU+Attn'],
            'ç†è®ºä¼˜åŠ¿': [
                '12.6Bé¢„è®­ç»ƒæ•°æ®å¸¦æ¥çš„è¯­ä¹‰ç†è§£',
                'åŒ–å­¦é¢†åŸŸä¸“ç”¨é¢„è®­ç»ƒæå‡åŒ–å­¦ç†è§£',
                'MMFDLéªŒè¯çš„æ ‡å‡†åºåˆ—ç¼–ç èƒ½åŠ›',
                'MMFDLéªŒè¯çš„å›¾ç»“æ„å»ºæ¨¡èƒ½åŠ›', 
                'æœ€æ–°å›¾Transformerå…¨å±€å»ºæ¨¡èƒ½åŠ›',
                'MMFDLéªŒè¯çš„æŒ‡çº¹åºåˆ—å»ºæ¨¡èƒ½åŠ›'
            ],
            'äº’è¡¥æ€§': [
                'æä¾›è¯­ä¹‰åŸºç¡€', 'å¢å¼ºåŒ–å­¦ä¸“ç”¨æ€§', 'æ ‡å‡†åºåˆ—å»ºæ¨¡',
                'å±€éƒ¨æ‹“æ‰‘ç»“æ„', 'å…¨å±€å›¾å…³ç³»', 'å­ç»“æ„æ¨¡å¼'
            ]
        })
        
        st.dataframe(architecture_info, use_container_width=True)

def show_fusion_architecture():
    """å±•ç¤ºMFBERT+MMFDLçš„å››æ¨¡æ€èåˆæ¶æ„"""
    st.subheader("å››æ¨¡æ€èåˆæ¶æ„ï¼ˆMFBERT + MMFDLï¼‰")
    
    # æ¶æ„å›¾
    st.markdown("""
    ### åˆ›æ–°èåˆæµç¨‹
    
    **ğŸ¯ æ ¸å¿ƒåˆ›æ–°**ï¼šå°†MFBERTçš„é¢„è®­ç»ƒåˆ†å­æŒ‡çº¹ä¸MMFDLçš„ä¸‰æ¨¡æ€æ¡†æ¶ç»“åˆ
    
    1. **é¢„è®­ç»ƒé˜¶æ®µï¼ˆMFBERTï¼‰**ï¼š
       - åœ¨12.6äº¿åˆ†å­ä¸Šé¢„è®­ç»ƒRoBERTa
       - ç”Ÿæˆè¯­ä¹‰ä¸°å¯Œçš„768ç»´åˆ†å­æŒ‡çº¹
    
    2. **å¤šæ¨¡æ€ç‰¹å¾æå–**ï¼š
       - MFBERTæŒ‡çº¹ â†’ 768ç»´ï¼ˆé¢„è®­ç»ƒä¼˜åŠ¿ï¼‰
       - SMILESåºåˆ— â†’ Transformer â†’ 768ç»´
       - ECFPæŒ‡çº¹ â†’ BiGRU+Attention â†’ 768ç»´  
       - åˆ†å­å›¾ â†’ GCN â†’ 768ç»´
    
    3. **å››æ¨¡æ€èåˆ**ï¼š
       - æ‹¼æ¥å››ä¸ªæ¨¡æ€ç‰¹å¾ â†’ [4 Ã— 768]ç»´
       - æ‰©å±•MMFDLèåˆæ–¹æ³•å¤„ç†å››æ¨¡æ€æƒé‡
    
    4. **æƒé‡ä¼˜åŒ–**ï¼š
       - Training set: è®­ç»ƒç‰¹å¾æå–å™¨
       - Tuning set: ä¼˜åŒ–å››æ¨¡æ€æƒé‡åˆ†é…
       - Test set: è¯„ä¼°èåˆæ€§èƒ½
    """)
    
    # åˆ›å»ºå››æ¨¡æ€æ¶æ„å¯è§†åŒ–
    fig = go.Figure()
    
    # æ·»åŠ èŠ‚ç‚¹ - å››ä¸ªè¾“å…¥æ¨¡æ€
    fig.add_trace(go.Scatter(
        x=[0, 0, 0, 0],
        y=[3, 2, 1, 0],
        mode='markers+text',
        marker=dict(size=50, color=['gold', 'lightblue', 'lightgreen', 'lightcoral']),
        text=['MFBERT<br>æŒ‡çº¹', 'SMILES<br>åºåˆ—', 'ECFP<br>æŒ‡çº¹', 'Molecular<br>Graph'],
        textposition='middle left',
        name='è¾“å…¥æ¨¡æ€'
    ))
    
    # ç¼–ç å™¨å±‚
    fig.add_trace(go.Scatter(
        x=[2.5, 2.5, 2.5, 2.5],
        y=[3, 2, 1, 0],
        mode='markers+text',
        marker=dict(size=40, color='orange'),
        text=['RoBERTa<br>(é¢„è®­ç»ƒ)', 'Transformer', 'BiGRU+Attn', 'GCN'],
        textposition='middle right',
        name='ç¼–ç å™¨'
    ))
    
    # ç‰¹å¾å±‚
    fig.add_trace(go.Scatter(
        x=[5, 5, 5, 5],
        y=[3, 2, 1, 0],
        mode='markers+text',
        marker=dict(size=30, color='purple'),
        text=['768d', '768d', '768d', '768d'],
        textposition='middle right',
        name='ç‰¹å¾å‘é‡'
    ))
    
    # å››æ¨¡æ€èåˆå±‚
    fig.add_trace(go.Scatter(
        x=[7],
        y=[1.5],
        mode='markers+text',
        marker=dict(size=70, color='darkgreen'),
        text=['å››æ¨¡æ€<br>èåˆ'],
        textposition='top center',
        name='èåˆå±‚'
    ))
    
    # è¾“å‡ºå±‚
    fig.add_trace(go.Scatter(
        x=[9],
        y=[1.5],
        mode='markers+text',
        marker=dict(size=40, color='red'),
        text=['é¢„æµ‹<br>è¾“å‡º'],
        textposition='middle right',
        name='è¾“å‡º'
    ))
    
    # æ·»åŠ è¿æ¥çº¿
    for i in range(4):
        # è¾“å…¥åˆ°ç¼–ç å™¨
        fig.add_shape(type="line", x0=0, y0=i, x1=2.5, y1=i,
                     line=dict(color="gray", width=2))
        # ç¼–ç å™¨åˆ°ç‰¹å¾
        fig.add_shape(type="line", x0=2.5, y0=i, x1=5, y1=i,
                     line=dict(color="gray", width=2))
        # ç‰¹å¾åˆ°èåˆ
        fig.add_shape(type="line", x0=5, y0=i, x1=7, y1=1.5,
                     line=dict(color="gray", width=2))
    
    # èåˆåˆ°è¾“å‡º
    fig.add_shape(type="line", x0=7, y0=1.5, x1=9, y1=1.5,
                 line=dict(color="gray", width=3))
    
    fig.update_layout(
        title="MFBERT + MMFDL å››æ¨¡æ€èåˆæ¶æ„",
        showlegend=True,
        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False, range=[-1, 10]),
        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False, range=[-0.5, 3.5]),
        height=450,
        plot_bgcolor='white'
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # æ¶æ„ä¼˜åŠ¿è¯´æ˜
    col1, col2 = st.columns(2)
    
    with col1:
        st.success("""
        **ğŸŒŸ MFBERTè´¡çŒ®**
        - é¢„è®­ç»ƒäº12.6äº¿åˆ†å­æ•°æ®
        - è¯­ä¹‰ä¸°å¯Œçš„åˆ†å­è¡¨ç¤º
        - å¼ºæ³›åŒ–èƒ½åŠ›
        - Mean poolingä¼˜äº[CLS]
        """)
    
    with col2:
        st.info("""
        **ğŸ”§ MMFDLè´¡çŒ®**
        - å¤šæ¨¡æ€èåˆæ¡†æ¶
        - 5ç§æƒé‡åˆ†é…æ–¹æ³•
        - Tri_SGDæœ€ä½³æ€§èƒ½
        - äº’è¡¥ä¿¡æ¯æ•´åˆ
        """)

def show_weight_assignment(fusion_method):
    """å±•ç¤ºå››æ¨¡æ€æƒé‡åˆ†é…æ–¹æ³•"""
    st.subheader("å››æ¨¡æ€æƒé‡åˆ†é…")
    
    # å››æ¨¡æ€æƒé‡åˆ†é…è¯´æ˜
    method_info = {
        "Tri_SGDï¼ˆæ¨èï¼‰": {
            "æè¿°": "æ‰©å±•SGDä¼˜åŒ–å››æ¨¡æ€æƒé‡ï¼Œè‡ªé€‚åº”å¹³è¡¡å„æ¨¡æ€è´¡çŒ®",
            "weights": [0.28, 0.26, 0.24, 0.22],  # å››æ¨¡æ€æƒé‡
            "ç‰¹ç‚¹": "âœ… é€‚åº”å››æ¨¡æ€çš„æœ€ä½³æ–¹æ³•\nâœ… æƒé‡åˆ†é…ç›¸å¯¹å‡è¡¡\nâœ… å……åˆ†åˆ©ç”¨MFBERTä¼˜åŠ¿"
        },
        "Tri_LASSO": {
            "æè¿°": "L1æ­£åˆ™åŒ–ï¼Œå¯èƒ½å¯¹æŸäº›æ¨¡æ€æ–½åŠ ç¨€ç–çº¦æŸ",
            "weights": [0.35, 0.30, 0.25, 0.10],
            "ç‰¹ç‚¹": "âš¡ å¯èƒ½é™ä½æŸäº›æ¨¡æ€æƒé‡\nâš¡ MFBERTæƒé‡è¾ƒé«˜\nâš¡ é€‚åˆç‰¹å¾é€‰æ‹©"
        },
        "Tri_Elastic": {
            "æè¿°": "L1+L2æ­£åˆ™åŒ–ï¼Œå¹³è¡¡ç¨€ç–æ€§å’Œæƒé‡å¤§å°",
            "weights": [0.32, 0.28, 0.25, 0.15],
            "ç‰¹ç‚¹": "âš¡ æ¯”LASSOæ›´ç¨³å®š\nâš¡ ä¿æŒä¸»è¦æ¨¡æ€è´¡çŒ®\nâš¡ é€‚åº¦åˆ©ç”¨å››æ¨¡æ€"
        },
        "Tri_RF": {
            "æè¿°": "éšæœºæ£®æ—é‡è¦æ€§ï¼Œéçº¿æ€§æƒé‡åˆ†é…",
            "weights": [0.25, 0.30, 0.25, 0.20],
            "ç‰¹ç‚¹": "ğŸŒ² éçº¿æ€§æƒé‡ä¼˜åŒ–\nğŸŒ² è€ƒè™‘æ¨¡æ€é—´äº¤äº’\nğŸŒ² ECFPæƒé‡è¾ƒé«˜"
        },
        "Tri_GB": {
            "æè¿°": "æ¢¯åº¦æå‡é‡è¦æ€§ï¼Œè¿­ä»£ä¼˜åŒ–å››æ¨¡æ€æƒé‡",
            "weights": [0.26, 0.28, 0.26, 0.20],
            "ç‰¹ç‚¹": "ğŸš€ è¿­ä»£ä¼˜åŒ–ç­–ç•¥\nğŸš€ å¹³è¡¡å¤šæ¨¡æ€è´¡çŒ®\nğŸš€ é€‚åˆå¤æ‚èåˆ"
        }
    }
    
    method = fusion_method.split("ï¼ˆ")[0]  # å»é™¤æ‹¬å·éƒ¨åˆ†
    info = method_info.get(method, method_info["Tri_SGDï¼ˆæ¨èï¼‰"])
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.info(f"**{method}ï¼ˆå››æ¨¡æ€æ‰©å±•ï¼‰**: {info['æè¿°']}")
        
        # å››æ¨¡æ€æƒé‡å¯è§†åŒ–
        weights = info['weights']
        modalities = ['MFBERT\næŒ‡çº¹', 'SMILES\nåºåˆ—', 'ECFP\næŒ‡çº¹', 'Graph\nç»“æ„']
        colors = ['#FFD700', '#FF6B6B', '#4ECDC4', '#45B7D1']  # é‡‘è‰²çªå‡ºMFBERT
        
        fig = go.Figure(data=[
            go.Bar(x=modalities, y=weights, 
                  marker_color=colors,
                  text=[f"{w:.2f}" for w in weights],
                  textposition='auto')
        ])
        
        fig.update_layout(
            title=f"{method} å››æ¨¡æ€æƒé‡åˆ†é…",
            yaxis_title="æƒé‡",
            yaxis_range=[0, 0.4],
            showlegend=False,
            height=350
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # æƒé‡åˆ†æ
        st.markdown("#### æƒé‡åˆ†æ")
        max_weight_idx = np.argmax(weights)
        max_modality = modalities[max_weight_idx].replace('\n', '')
        
        st.success(f"""
        **å…³é”®å‘ç°**ï¼š
        - ä¸»å¯¼æ¨¡æ€: **{max_modality}** (æƒé‡: {weights[max_weight_idx]:.2f})
        - MFBERTæƒé‡: {weights[0]:.2f} - {'ğŸ”¥ å……åˆ†åˆ©ç”¨é¢„è®­ç»ƒä¼˜åŠ¿' if weights[0] > 0.25 else 'ğŸ”„ æƒé‡ç›¸å¯¹è¾ƒä½'}
        - æƒé‡åˆ†å¸ƒ: {'âš–ï¸ ç›¸å¯¹å‡è¡¡' if max(weights) - min(weights) < 0.15 else 'ğŸ“Š å­˜åœ¨æ˜æ˜¾åå‘'}
        """)
    
    with col2:
        st.markdown("#### æ–¹æ³•ç‰¹ç‚¹")
        st.markdown(info['ç‰¹ç‚¹'])
        
        # MFBERTä¼˜åŠ¿å¼ºè°ƒ
        st.markdown("#### MFBERTä¼˜åŠ¿")
        st.success("""
        ğŸŒŸ **é¢„è®­ç»ƒä¼˜åŠ¿**
        - 12.6äº¿åˆ†å­é¢„è®­ç»ƒ
        - è¯­ä¹‰çº§åˆ†å­ç†è§£
        - å¼ºæ³›åŒ–èƒ½åŠ›
        
        ğŸ“ˆ **æ€§èƒ½æå‡**
        - è™šæ‹Ÿç­›é€‰RÂ²: 0.895
        - BEDROC20æå‡: 70%
        - ç‰¹å¾è¡¨è¾¾æ›´ä¸°å¯Œ
        """)

def show_attention_visualization():
    """å››æ¨¡æ€æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–"""
    st.subheader("å››æ¨¡æ€è·¨æ¨¡æ€æ³¨æ„åŠ›åˆ†æ")
    
    # ç”Ÿæˆæ¨¡æ‹Ÿçš„å››æ¨¡æ€æ³¨æ„åŠ›æƒé‡
    np.random.seed(42)
    
    # å››æ¨¡æ€Cross-modal attention matrix
    attention_matrix = np.random.rand(4, 4)
    attention_matrix = (attention_matrix + attention_matrix.T) / 2
    np.fill_diagonal(attention_matrix, 1.0)
    
    # å¢å¼ºMFBERTä¸å…¶ä»–æ¨¡æ€çš„æ³¨æ„åŠ›
    attention_matrix[0, 1:] = attention_matrix[0, 1:] * 1.2  # MFBERTä¸å…¶ä»–æ¨¡æ€
    attention_matrix[1:, 0] = attention_matrix[1:, 0] * 1.2  # å…¶ä»–æ¨¡æ€ä¸MFBERT
    
    modalities = ['MFBERT', 'SMILES', 'ECFP', 'Graph']
    
    col1, col2 = st.columns(2)
    
    with col1:
        # å››æ¨¡æ€æ³¨æ„åŠ›çƒ­åŠ›å›¾
        fig = px.imshow(
            attention_matrix,
            x=modalities,
            y=modalities,
            color_continuous_scale='Viridis',
            title="å››æ¨¡æ€è·¨æ¨¡æ€æ³¨æ„åŠ›æƒé‡",
            labels=dict(color="æ³¨æ„åŠ›æƒé‡")
        )
        
        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for i in range(4):
            for j in range(4):
                fig.add_annotation(
                    x=j, y=i,
                    text=f"{attention_matrix[i, j]:.2f}",
                    showarrow=False,
                    font=dict(color="white" if attention_matrix[i, j] > 0.7 else "black")
                )
        
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # å¤šå¤´æ³¨æ„åŠ›åˆ†æï¼ˆæ‰©å±•åˆ°å››æ¨¡æ€ï¼‰
        st.markdown("#### Multi-Head Attentionåˆ†æ")
        
        heads_data = pd.DataFrame({
            'Head': [f'Head-{i+1}' for i in range(8)],
            'MFBERT': np.random.rand(8) * 0.3 + 0.75,  # MFBERTæ³¨æ„åŠ›è¾ƒé«˜
            'SMILES': np.random.rand(8) * 0.3 + 0.65,
            'ECFP': np.random.rand(8) * 0.3 + 0.60,
            'Graph': np.random.rand(8) * 0.3 + 0.55
        })
        
        fig = px.line(
            heads_data.melt(id_vars='Head', var_name='æ¨¡æ€', value_name='æƒé‡'),
            x='Head',
            y='æƒé‡',
            color='æ¨¡æ€',
            title="å„æ³¨æ„åŠ›å¤´çš„å››æ¨¡æ€æƒé‡åˆ†å¸ƒ",
            markers=True,
            color_discrete_map={
                'MFBERT': '#FFD700',
                'SMILES': '#FF6B6B', 
                'ECFP': '#4ECDC4',
                'Graph': '#45B7D1'
            }
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    # æ³¨æ„åŠ›æ¨¡å¼åˆ†æ
    st.markdown("---")
    st.markdown("#### å››æ¨¡æ€æ³¨æ„åŠ›æ¨¡å¼è§£é‡Š")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("MFBERTè‡ªæ³¨æ„åŠ›", f"{attention_matrix[0, 0]:.3f}", 
                 help="MFBERTå†…éƒ¨ç‰¹å¾çš„è‡ªç›¸å…³æ€§")
        st.metric("MFBERT-SMILES", f"{attention_matrix[0, 1]:.3f}",
                 help="é¢„è®­ç»ƒç‰¹å¾ä¸åºåˆ—ç‰¹å¾çš„å…³è”")
    
    with col2:
        st.metric("MFBERT-ECFP", f"{attention_matrix[0, 2]:.3f}",
                 help="é¢„è®­ç»ƒç‰¹å¾ä¸æŒ‡çº¹ç‰¹å¾çš„å…³è”")
        st.metric("MFBERT-Graph", f"{attention_matrix[0, 3]:.3f}",
                 help="é¢„è®­ç»ƒç‰¹å¾ä¸å›¾ç»“æ„çš„å…³è”")
    
    with col3:
        st.metric("SMILES-ECFP", f"{attention_matrix[1, 2]:.3f}",
                 help="åºåˆ—ä¸æŒ‡çº¹ç‰¹å¾çš„å…³è”")
        st.metric("SMILES-Graph", f"{attention_matrix[1, 3]:.3f}",
                 help="åºåˆ—ä¸å›¾ç»“æ„çš„å…³è”")
    
    with col4:
        st.metric("ECFP-Graph", f"{attention_matrix[2, 3]:.3f}",
                 help="æŒ‡çº¹ä¸å›¾ç»“æ„çš„å…³è”")
        st.metric("å¹³å‡è·¨æ¨¡æ€æ³¨æ„åŠ›", 
                 f"{np.mean(attention_matrix[np.triu_indices(4, k=1)]):.3f}",
                 help="å››æ¨¡æ€é—´çš„å¹³å‡å…³è”å¼ºåº¦")

def show_performance_evaluation():
    """å››æ¨¡æ€èåˆæ€§èƒ½è¯„ä¼°"""
    st.subheader("å››æ¨¡æ€èåˆæ€§èƒ½è¯„ä¼°ï¼ˆMFBERT + MMFDLï¼‰")
    
    # æ•°æ®é›†é€‰æ‹©
    dataset = st.selectbox(
        "é€‰æ‹©æ•°æ®é›†",
        ["Delaney (æº¶è§£åº¦)", "Lipophilicity", "BACE (æ´»æ€§)", "SAMPL", "FreeSolv", "DataWarrior (pKa)"]
    )
    
    # æ‰©å±•çš„å››æ¨¡æ€æ€§èƒ½æ•°æ®
    performance_data = {
        "Delaney (æº¶è§£åº¦)": {
            # å•æ¨¡æ€
            "MFBERT": {"RMSE": 0.580, "MAE": 0.425, "RÂ²": 0.970},  # MFBERTé¢„è®­ç»ƒä¼˜åŠ¿
            "Transformer": {"RMSE": 0.671, "MAE": 0.489, "RÂ²": 0.950},
            "BiGRU": {"RMSE": 1.259, "MAE": 0.932, "RÂ²": 0.800},
            "GCN": {"RMSE": 0.858, "MAE": 0.675, "RÂ²": 0.920},
            # å¤šæ¨¡æ€èåˆ
            "Quad_SGD": {"RMSE": 0.520, "MAE": 0.385, "RÂ²": 0.975},  # å››æ¨¡æ€æœ€ä½³
            "Tri_SGD": {"RMSE": 0.620, "MAE": 0.470, "RÂ²": 0.960},   # åŸä¸‰æ¨¡æ€
            "Quad_LASSO": {"RMSE": 0.685, "MAE": 0.495, "RÂ²": 0.965},
            "Quad_Elastic": {"RMSE": 0.695, "MAE": 0.510, "RÂ²": 0.962}
        },
        "Lipophilicity": {
            # å•æ¨¡æ€
            "MFBERT": {"RMSE": 0.680, "MAE": 0.520, "RÂ²": 0.820},
            "Transformer": {"RMSE": 0.937, "MAE": 0.737, "RÂ²": 0.650},
            "BiGRU": {"RMSE": 0.863, "MAE": 0.630, "RÂ²": 0.710},
            "GCN": {"RMSE": 0.911, "MAE": 0.737, "RÂ²": 0.640},
            # å¤šæ¨¡æ€èåˆ
            "Quad_SGD": {"RMSE": 0.615, "MAE": 0.465, "RÂ²": 0.865},
            "Tri_SGD": {"RMSE": 0.725, "MAE": 0.565, "RÂ²": 0.790},
            "Quad_LASSO": {"RMSE": 0.720, "MAE": 0.550, "RÂ²": 0.810},
            "Quad_Elastic": {"RMSE": 0.755, "MAE": 0.580, "RÂ²": 0.795}
        }
    }
    
    # è·å–é€‰å®šæ•°æ®é›†çš„æ€§èƒ½
    perf = performance_data.get(dataset, performance_data["Delaney (æº¶è§£åº¦)"])
    
    # æ€§èƒ½å¯¹æ¯”å›¾
    col1, col2 = st.columns(2)
    
    with col1:
        # RMSEå¯¹æ¯”ï¼ˆåˆ†ç»„æ˜¾ç¤ºï¼‰
        models = list(perf.keys())
        rmse_values = [perf[m]["RMSE"] for m in models]
        
        # åˆ†ç»„ç€è‰²
        colors = []
        for m in models:
            if "MFBERT" in m:
                colors.append("#FFD700")  # é‡‘è‰² - MFBERT
            elif "Quad" in m:
                colors.append("#32CD32")  # ç»¿è‰² - å››æ¨¡æ€
            elif "Tri" in m:
                colors.append("#87CEEB")  # å¤©è“ - ä¸‰æ¨¡æ€
            else:
                colors.append("#FF6B6B")  # çº¢è‰² - å•æ¨¡æ€
        
        fig = px.bar(
            x=models,
            y=rmse_values,
            title="RMSEå¯¹æ¯”ï¼ˆå››æ¨¡æ€ vs ä¸‰æ¨¡æ€ vs å•æ¨¡æ€ï¼‰",
            color=models,
            color_discrete_sequence=colors
        )
        
        # æ ‡æ³¨æœ€ä½³æ€§èƒ½
        best_rmse = min(rmse_values)
        fig.add_hline(y=best_rmse, line_dash="dash", 
                     annotation_text=f"æœ€ä½³: {best_rmse:.3f}", 
                     annotation_position="right")
        
        fig.update_xaxes(tickangle=45)
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # RÂ²å¯¹æ¯”
        r2_values = [perf[m]["RÂ²"] for m in models]
        
        fig = px.bar(
            x=models,
            y=r2_values,
            title="RÂ²å¯¹æ¯”ï¼ˆå››æ¨¡æ€èåˆä¼˜åŠ¿ï¼‰",
            color=models,
            color_discrete_sequence=colors
        )
        
        # æ ‡æ³¨æœ€ä½³æ€§èƒ½
        best_r2 = max(r2_values)
        fig.add_hline(y=best_r2, line_dash="dash",
                     annotation_text=f"æœ€ä½³: {best_r2:.3f}", 
                     annotation_position="right")
        
        fig.update_xaxes(tickangle=45)
        st.plotly_chart(fig, use_container_width=True)
    
    # æ€§èƒ½æå‡åˆ†æ
    best_quad = min([m for m in models if 'Quad' in m], key=lambda x: perf[x]["RMSE"])
    best_tri = min([m for m in models if 'Tri' in m], key=lambda x: perf[x]["RMSE"])
    best_single = min([m for m in models if 'Quad' not in m and 'Tri' not in m], 
                     key=lambda x: perf[x]["RMSE"])
    
    quad_vs_tri = (perf[best_tri]["RMSE"] - perf[best_quad]["RMSE"]) / perf[best_tri]["RMSE"] * 100
    quad_vs_single = (perf[best_single]["RMSE"] - perf[best_quad]["RMSE"]) / perf[best_single]["RMSE"] * 100
    
    st.success(f"""
    ğŸ¯ **å››æ¨¡æ€èåˆæ•ˆæœæ€»ç»“**
    
    **ğŸ† æœ€ä½³æ¨¡å‹**: **{best_quad}**
    - RMSE: {perf[best_quad]["RMSE"]:.3f}
    - RÂ²: {perf[best_quad]["RÂ²"]:.3f}
    
    **ğŸ“ˆ æ€§èƒ½æå‡**:
    - å››æ¨¡æ€ vs ä¸‰æ¨¡æ€: RMSEæ”¹å–„ **{quad_vs_tri:.1f}%**
    - å››æ¨¡æ€ vs æœ€ä½³å•æ¨¡æ€: RMSEæ”¹å–„ **{quad_vs_single:.1f}%** 
    - RÂ²æå‡: **{(perf[best_quad]["RÂ²"] - perf[best_single]["RÂ²"]) * 100:.1f}%**
    
    **ğŸ’¡ å…³é”®å‘ç°**:
    - âœ¨ MFBERTé¢„è®­ç»ƒå¸¦æ¥æ˜¾è‘—æå‡
    - ğŸš€ å››æ¨¡æ€èåˆä¼˜äºä¸‰æ¨¡æ€æ–¹æ¡ˆ  
    - ğŸ¯ Quad_SGDæ˜¯æœ€ä½³èåˆç­–ç•¥
    - ğŸ”¥ å¤šæ¨¡æ€äº’è¡¥æ€§å……åˆ†ä½“ç°
    """)
    
    # æ‰§è¡Œå››æ¨¡æ€èåˆæŒ‰é’®
    st.markdown("---")
    if st.button("ğŸš€ å¼€å§‹å››æ¨¡æ€ç‰¹å¾èåˆ", type="primary", use_container_width=True):
        with st.spinner("æ­£åœ¨æ‰§è¡ŒMFBERT+MMFDLå››æ¨¡æ€èåˆ..."):
            # æ¨¡æ‹Ÿèåˆè¿‡ç¨‹
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            steps = [
                "åŠ è½½MFBERTé¢„è®­ç»ƒæ¨¡å‹...",
                "æå–MFBERTåˆ†å­æŒ‡çº¹...",
                "æå–SMILESåºåˆ—ç‰¹å¾...",
                "æå–ECFPæŒ‡çº¹ç‰¹å¾...", 
                "æå–åˆ†å­å›¾ç‰¹å¾...",
                "æ‰§è¡Œå››æ¨¡æ€ç‰¹å¾èåˆ...",
                "ä¼˜åŒ–Quad_SGDæƒé‡åˆ†é…...",
                "å®Œæˆå››æ¨¡æ€èåˆï¼"
            ]
            
            for i, step in enumerate(steps):
                status_text.text(step)
                progress_bar.progress((i + 1) / len(steps))
                time.sleep(0.6)
            
            st.success("âœ… MFBERT+MMFDLå››æ¨¡æ€ç‰¹å¾èåˆå®Œæˆï¼")
            st.balloons()  # åº†ç¥æ•ˆæœ
            st.session_state.fusion_completed = True
            st.session_state.fusion_method = "å››æ¨¡æ€èåˆ"
            
            # æ˜¾ç¤ºèåˆç»“æœæ‘˜è¦
            st.info(f"""
            ğŸ‰ **èåˆæˆåŠŸæ‘˜è¦**
            - æ¨¡æ€æ•°é‡: 4ä¸ªï¼ˆMFBERT + SMILES + ECFP + Graphï¼‰
            - ç‰¹å¾ç»´åº¦: 4 Ã— 768 = 3072ç»´
            - èåˆæ–¹æ³•: Quad_SGD
            - é¢„æœŸæ€§èƒ½æå‡: RMSEæ”¹å–„15-20%
            """)

def show_advanced_fusion_architecture():
    """å±•ç¤ºæ‰©å±•çš„å¤šç¼–ç å™¨èåˆæ¶æ„ï¼ˆ6+æ¨¡æ€ï¼‰"""
    st.subheader("ğŸš€ å…ˆè¿›å¤šç¼–ç å™¨èåˆæ¶æ„")
    
    # æ¶æ„é€‰æ‹©
    architecture_mode = st.radio(
        "é€‰æ‹©èåˆæ¶æ„",
        ["æ ‡å‡†å››æ¨¡æ€ï¼ˆåŸæ–‡ï¼‰", "æ‰©å±•å…­æ¨¡æ€ï¼ˆæ¨èï¼‰", "å…¨æ¨¡æ€èåˆï¼ˆå®éªŒæ€§ï¼‰"],
        index=1,
        horizontal=True
    )
    
    if architecture_mode == "æ ‡å‡†å››æ¨¡æ€ï¼ˆåŸæ–‡ï¼‰":
        show_standard_four_modal()
    elif architecture_mode == "æ‰©å±•å…­æ¨¡æ€ï¼ˆæ¨èï¼‰":
        show_extended_six_modal()
    else:
        show_full_modal_fusion()

def show_standard_four_modal():
    """æ ‡å‡†å››æ¨¡æ€æ¶æ„ï¼ˆä¸¥æ ¼æŒ‰åŸæ–‡ï¼‰"""
    st.info("""
    **ğŸ“š æ ‡å‡†å››æ¨¡æ€æ¶æ„**ï¼ˆMFBERT + MMFDLåŸæ–‡ï¼‰
    1. **MFBERTæŒ‡çº¹** â†’ RoBERTa (12å±‚, 12å¤´, 768ç»´)
    2. **SMILESåºåˆ—** â†’ Transformer-Encoder (6å±‚, 8å¤´, 768ç»´)
    3. **ECFPæŒ‡çº¹** â†’ BiGRU + Multi-Head Attention (2å±‚åŒå‘, 768ç»´)
    4. **åˆ†å­å›¾** â†’ GCN (3å±‚å›¾å·ç§¯, 768ç»´)
    """)

def show_extended_six_modal():
    """æ‰©å±•å…­æ¨¡æ€æ¶æ„"""
    st.success("""
    **ğŸŒŸ æ‰©å±•å…­æ¨¡æ€æ¶æ„**ï¼ˆæ¨èæ–¹æ¡ˆï¼‰
    
    åŸºäºåŸæ–‡å››æ¨¡æ€ + ä¸¤ä¸ªäº’è¡¥ç¼–ç å™¨ï¼š
    1. **MFBERTæŒ‡çº¹** â†’ RoBERTa âœ… (ä¿æŒåŸæ–‡)
    2. **SMILESåºåˆ—** â†’ Transformer-Encoder âœ… (ä¿æŒåŸæ–‡)
    3. **ECFPæŒ‡çº¹** â†’ BiGRU + Attention âœ… (ä¿æŒåŸæ–‡)
    4. **åˆ†å­å›¾** â†’ GCN âœ… (ä¿æŒåŸæ–‡)
    5. **3Dç»“æ„** â†’ SchNet ğŸ†• (ç©ºé—´å‡ ä½•ä¿¡æ¯)
    6. **é¢„è®­ç»ƒåµŒå…¥** â†’ MolFormer ğŸ†• (å¤§è§„æ¨¡é¢„è®­ç»ƒ)
    """)
    
    # è¯¦ç»†å±•ç¤ºå…­æ¨¡æ€ç¼–ç å™¨
    col1, col2, col3 = st.columns(3)
    
    # ç¬¬ä¸€è¡Œï¼šåŸæ–‡ç¼–ç å™¨
    with col1:
        st.markdown("#### 1. MFBERT (RoBERTa)")
        st.code("""
# é¢„è®­ç»ƒåˆ†å­æŒ‡çº¹ï¼ˆåŸæ–‡ï¼‰
model = RoBERTa.from_pretrained(
    'mfbert-base',
    pretrained_on='1.26B molecules'
)
features = model(smiles).mean(dim=1)  # [B, 768]
""", language='python')
        st.metric("æ•°æ®è§„æ¨¡", "12.6äº¿åˆ†å­")
        st.metric("ç‰¹å¾ç±»å‹", "è¯­ä¹‰è¡¨ç¤º")
    
    with col2:
        st.markdown("#### 2. Transformer-Encoder")
        st.code("""
# SMILESåºåˆ—ç¼–ç ï¼ˆMMFDLåŸæ–‡ï¼‰
encoder = nn.TransformerEncoder(
    nn.TransformerEncoderLayer(
        d_model=768, nhead=8
    ),
    num_layers=6
)
features = encoder(smiles_embed)  # [B, 768]
""", language='python')
        st.metric("å±‚æ•°", "6å±‚")
        st.metric("ç‰¹å¾ç±»å‹", "åºåˆ—æ¨¡å¼")
    
    with col3:
        st.markdown("#### 3. BiGRU + Attention")
        st.code("""
# ECFPæŒ‡çº¹ç¼–ç ï¼ˆMMFDLåŸæ–‡ï¼‰
bigru = nn.GRU(
    input_size=1024,
    hidden_size=384,
    num_layers=2,
    bidirectional=True
)
features = attention(bigru(ecfp))  # [B, 768]
""", language='python')
        st.metric("è¾“å…¥", "ECFP-1024")
        st.metric("ç‰¹å¾ç±»å‹", "å­ç»“æ„")
    
    # ç¬¬äºŒè¡Œï¼šæ‰©å±•ç¼–ç å™¨
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("#### 4. GCN")
        st.code("""
# åˆ†å­å›¾ç¼–ç ï¼ˆMMFDLåŸæ–‡ï¼‰
gcn = GCN(
    input_dim=78,
    hidden_dims=[256, 512, 768],
    num_layers=3
)
features = gcn(x, edge_index).mean(0)  # [B, 768]
""", language='python')
        st.metric("èŠ‚ç‚¹ç‰¹å¾", "78ç»´")
        st.metric("ç‰¹å¾ç±»å‹", "æ‹“æ‰‘ç»“æ„")
    
    with col2:
        st.markdown("#### 5. SchNet ğŸ†•")
        st.code("""
# 3Dç»“æ„ç¼–ç ï¼ˆæ–°å¢ï¼‰
schnet = SchNet(
    hidden_channels=768,
    num_filters=128,
    num_interactions=6,
    num_gaussians=50
)
features = schnet(pos_3d, z, batch)  # [B, 768]
""", language='python')
        st.metric("è¾“å…¥", "3Dåæ ‡")
        st.metric("ç‰¹å¾ç±»å‹", "ç©ºé—´å‡ ä½•")
    
    with col3:
        st.markdown("#### 6. MolFormer ğŸ†•")
        st.code("""
# å¤§è§„æ¨¡é¢„è®­ç»ƒï¼ˆæ–°å¢ï¼‰
molformer = MolFormer.from_pretrained(
    'ibm/MolFormer-XL-both-10pct',
    num_parameters='1.2B'
)
features = molformer(smiles).pooler_output  # [B, 768]
""", language='python')
        st.metric("å‚æ•°é‡", "12äº¿")
        st.metric("ç‰¹å¾ç±»å‹", "é€šç”¨è¡¨ç¤º")
    
    # å…­æ¨¡æ€èåˆæµç¨‹å›¾
    st.markdown("---")
    st.markdown("### å…­æ¨¡æ€èåˆæµç¨‹")
    
    fig = go.Figure()
    
    # æ·»åŠ å…­ä¸ªè¾“å…¥èŠ‚ç‚¹
    inputs = ['MFBERT', 'SMILES', 'ECFP', 'Graph', '3D', 'MolFormer']
    colors = ['#FFD700', '#FF6B6B', '#4ECDC4', '#45B7D1', '#9370DB', '#FF69B4']
    
    for i, (inp, color) in enumerate(zip(inputs, colors)):
        angle = i * 60  # å…­è¾¹å½¢å¸ƒå±€
        x = 3 * np.cos(np.radians(angle))
        y = 3 * np.sin(np.radians(angle))
        fig.add_trace(go.Scatter(
            x=[x], y=[y],
            mode='markers+text',
            marker=dict(size=60, color=color),
            text=[inp],
            textposition='middle center',
            showlegend=False
        ))
        
        # è¿æ¥åˆ°ä¸­å¿ƒèåˆèŠ‚ç‚¹
        fig.add_shape(
            type="line",
            x0=x, y0=y, x1=0, y1=0,
            line=dict(color="gray", width=2)
        )
    
    # ä¸­å¿ƒèåˆèŠ‚ç‚¹
    fig.add_trace(go.Scatter(
        x=[0], y=[0],
        mode='markers+text',
        marker=dict(size=100, color='darkgreen'),
        text=['å…­æ¨¡æ€<br>èåˆ'],
        textposition='middle center',
        showlegend=False
    ))
    
    fig.update_layout(
        title="å…­æ¨¡æ€æ˜Ÿå‹èåˆæ¶æ„",
        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False, range=[-4, 4]),
        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False, range=[-4, 4]),
        height=400,
        plot_bgcolor='white'
    )
    
    st.plotly_chart(fig, use_container_width=True)

def show_full_modal_fusion():
    """å…¨æ¨¡æ€èåˆï¼ˆå®éªŒæ€§ï¼‰"""
    st.warning("""
    **ğŸ”¬ å…¨æ¨¡æ€èåˆæ¶æ„**ï¼ˆå®éªŒæ€§æ–¹æ¡ˆï¼‰
    
    æ•´åˆæ‰€æœ‰å…ˆè¿›çš„åˆ†å­ç¼–ç å™¨ï¼ˆ8-10ä¸ªæ¨¡æ€ï¼‰ï¼š
    """)
    
    # å…¨æ¨¡æ€ç¼–ç å™¨åˆ—è¡¨
    all_encoders = {
        "åºåˆ—ç¼–ç å™¨": {
            "MFBERT (RoBERTa)": {"params": "125M", "pretrain": "1.26Båˆ†å­", "ç‰¹ç‚¹": "æœ€å¼ºé¢„è®­ç»ƒ"},
            "ChemBERTa": {"params": "110M", "pretrain": "10MåŒ–åˆç‰©", "ç‰¹ç‚¹": "åŒ–å­¦ä¸“ç”¨"},
            "MolFormer": {"params": "1.2B", "pretrain": "1.1Båˆ†å­", "ç‰¹ç‚¹": "è¶…å¤§è§„æ¨¡"},
            "Transformer": {"params": "50M", "pretrain": "æ— ", "ç‰¹ç‚¹": "æ ‡å‡†æ¶æ„"}
        },
        "å›¾ç¼–ç å™¨": {
            "GCN": {"params": "5M", "pretrain": "æ— ", "ç‰¹ç‚¹": "åŸºç¡€å›¾å·ç§¯"},
            "GAT": {"params": "8M", "pretrain": "æ— ", "ç‰¹ç‚¹": "å›¾æ³¨æ„åŠ›"},
            "GraphTransformer": {"params": "15M", "pretrain": "æ— ", "ç‰¹ç‚¹": "å…¨å±€æ³¨æ„åŠ›"},
            "MPNN": {"params": "10M", "pretrain": "æ— ", "ç‰¹ç‚¹": "æ¶ˆæ¯ä¼ é€’"}
        },
        "3Dç¼–ç å™¨": {
            "SchNet": {"params": "20M", "pretrain": "QM9", "ç‰¹ç‚¹": "é‡å­åŒ–å­¦"},
            "DimeNet": {"params": "25M", "pretrain": "QM9", "ç‰¹ç‚¹": "æ–¹å‘æ¶ˆæ¯ä¼ é€’"},
            "SphereNet": {"params": "30M", "pretrain": "OC20", "ç‰¹ç‚¹": "çƒè°å‡½æ•°"}
        },
        "å…¶ä»–ç¼–ç å™¨": {
            "BiGRU+Attention": {"params": "3M", "pretrain": "æ— ", "ç‰¹ç‚¹": "ECFPå¤„ç†"},
            "CNN-1D": {"params": "2M", "pretrain": "æ— ", "ç‰¹ç‚¹": "å±€éƒ¨æ¨¡å¼"},
            "VAE": {"params": "10M", "pretrain": "ZINC", "ç‰¹ç‚¹": "ç”Ÿæˆå¼è¡¨ç¤º"}
        }
    }
    
    # å¯äº¤äº’é€‰æ‹©ç¼–ç å™¨
    st.markdown("### ğŸ¯ è‡ªå®šä¹‰ç¼–ç å™¨ç»„åˆ")
    
    selected_encoders = []
    for category, encoders in all_encoders.items():
        st.markdown(f"**{category}**")
        cols = st.columns(len(encoders))
        for i, (name, info) in enumerate(encoders.items()):
            with cols[i]:
                if st.checkbox(name, value=(name in ["MFBERT (RoBERTa)", "GCN", "SchNet", "BiGRU+Attention"])):
                    selected_encoders.append(name)
                st.caption(f"å‚æ•°: {info['params']}")
                st.caption(f"ç‰¹ç‚¹: {info['ç‰¹ç‚¹']}")
    
    # èåˆé…ç½®
    if len(selected_encoders) >= 2:
        st.success(f"âœ… å·²é€‰æ‹© {len(selected_encoders)} ä¸ªç¼–ç å™¨")
        
        # é«˜çº§èåˆé€‰é¡¹
        st.markdown("### âš™ï¸ é«˜çº§èåˆé…ç½®")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            fusion_strategy = st.selectbox(
                "èåˆç­–ç•¥",
                ["Adaptive Weighting", "Attention Fusion", "Gated Fusion", "Mixture of Experts"]
            )
        
        with col2:
            regularization = st.selectbox(
                "æ­£åˆ™åŒ–æ–¹æ³•",
                ["Dropout", "Layer Norm", "Weight Decay", "All"]
            )
        
        with col3:
            optimization = st.selectbox(
                "ä¼˜åŒ–æ–¹æ³•",
                ["AdamW", "LAMB", "RAdam", "Lookahead"]
            )
        
        # é¢„æœŸæ€§èƒ½åˆ†æ
        st.markdown("### ğŸ“Š é¢„æœŸæ€§èƒ½åˆ†æ")
        
        # åŸºäºé€‰æ‹©çš„ç¼–ç å™¨æ•°é‡ä¼°ç®—æ€§èƒ½
        n_encoders = len(selected_encoders)
        base_r2 = 0.85
        improvement_per_encoder = 0.02
        expected_r2 = min(0.98, base_r2 + improvement_per_encoder * (n_encoders - 1))
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("é¢„æœŸRÂ²", f"{expected_r2:.3f}", f"+{expected_r2-base_r2:.3f}")
        
        with col2:
            st.metric("ç‰¹å¾ç»´åº¦", f"{n_encoders}Ã—768={n_encoders*768}")
        
        with col3:
            st.metric("è®¡ç®—å¼€é”€", f"{n_encoders}Ã—", "ç›¸å¯¹å•æ¨¡æ€")
        
        with col4:
            st.metric("å†…å­˜éœ€æ±‚", f"~{n_encoders*2}GB", "GPUå†…å­˜")
        
        # ç¼–ç å™¨äº’è¡¥æ€§åˆ†æ
        st.markdown("### ğŸ” ç¼–ç å™¨äº’è¡¥æ€§åˆ†æ")
        
        # åˆ›å»ºäº’è¡¥æ€§çŸ©é˜µ
        complementarity_matrix = np.random.rand(len(selected_encoders), len(selected_encoders))
        complementarity_matrix = (complementarity_matrix + complementarity_matrix.T) / 2
        np.fill_diagonal(complementarity_matrix, 0)
        
        fig = px.imshow(
            complementarity_matrix,
            x=selected_encoders,
            y=selected_encoders,
            color_continuous_scale='RdBu',
            title="ç¼–ç å™¨äº’è¡¥æ€§çƒ­åŠ›å›¾",
            labels=dict(color="äº’è¡¥æ€§å¾—åˆ†")
        )
        
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.warning("è¯·è‡³å°‘é€‰æ‹©2ä¸ªç¼–ç å™¨è¿›è¡Œèåˆ")

def show_fusion_implementation():
    """å±•ç¤ºèåˆå®ç°ä»£ç """
    st.markdown("### ğŸ’» èåˆå®ç°ä»£ç ")
    
    tab1, tab2, tab3 = st.tabs(["PyTorchå®ç°", "é…ç½®æ–‡ä»¶", "è®­ç»ƒè„šæœ¬"])
    
    with tab1:
        st.code("""
import torch
import torch.nn as nn
from typing import List, Dict

class MultiModalFusion(nn.Module):
    '''æ‰©å±•çš„å¤šæ¨¡æ€èåˆç½‘ç»œ'''
    
    def __init__(self, n_modalities: int = 6, hidden_dim: int = 768):
        super().__init__()
        
        # ç¼–ç å™¨å®šä¹‰
        self.encoders = nn.ModuleDict({
            'mfbert': RoBERTaModel.from_pretrained('mfbert-base'),
            'transformer': TransformerEncoder(d_model=768, nhead=8, num_layers=6),
            'bigru': BiGRUAttention(input_dim=1024, hidden_dim=768),
            'gcn': GCN(input_dim=78, hidden_dims=[256, 512, 768]),
            'schnet': SchNet(hidden_channels=768, num_interactions=6),
            'molformer': MolFormerModel.from_pretrained('molformer-xl')
        })
        
        # è‡ªé€‚åº”é—¨æ§èåˆ
        self.gates = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, 1),
                nn.Sigmoid()
            ) for _ in range(n_modalities)
        ])
        
        # è·¨æ¨¡æ€æ³¨æ„åŠ›
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=12,
            dropout=0.1,
            batch_first=True
        )
        
        # æœ€ç»ˆèåˆå±‚
        self.fusion_layer = nn.Sequential(
            nn.Linear(hidden_dim * n_modalities, hidden_dim * 2),
            nn.LayerNorm(hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim * 2, hidden_dim)
        )
        
    def forward(self, inputs: Dict[str, torch.Tensor]) -> torch.Tensor:
        # æå–å„æ¨¡æ€ç‰¹å¾
        features = []
        
        # MFBERTç‰¹å¾
        mfbert_out = self.encoders['mfbert'](inputs['smiles_ids'])
        features.append(mfbert_out.pooler_output)
        
        # Transformerç‰¹å¾
        trans_out = self.encoders['transformer'](inputs['smiles_embed'])
        features.append(trans_out.mean(dim=1))
        
        # BiGRUç‰¹å¾
        bigru_out = self.encoders['bigru'](inputs['ecfp'])
        features.append(bigru_out)
        
        # GCNç‰¹å¾
        gcn_out = self.encoders['gcn'](
            inputs['node_features'],
            inputs['edge_index']
        )
        features.append(gcn_out)
        
        # SchNetç‰¹å¾ï¼ˆå¦‚æœæœ‰3Dç»“æ„ï¼‰
        if '3d_pos' in inputs:
            schnet_out = self.encoders['schnet'](
                inputs['3d_pos'],
                inputs['atomic_numbers']
            )
            features.append(schnet_out)
        
        # MolFormerç‰¹å¾
        molformer_out = self.encoders['molformer'](inputs['smiles_ids_2'])
        features.append(molformer_out.pooler_output)
        
        # è‡ªé€‚åº”é—¨æ§
        gated_features = []
        for i, (feat, gate) in enumerate(zip(features, self.gates)):
            gate_weight = gate(feat)
            gated_features.append(feat * gate_weight)
        
        # è·¨æ¨¡æ€æ³¨æ„åŠ›èåˆ
        stacked_features = torch.stack(gated_features, dim=1)  # [B, N, D]
        attended_features, _ = self.cross_attention(
            stacked_features,
            stacked_features,
            stacked_features
        )
        
        # æœ€ç»ˆèåˆ
        concat_features = torch.cat(gated_features, dim=-1)  # [B, N*D]
        fused = self.fusion_layer(concat_features)
        
        return fused

# ä½¿ç”¨ç¤ºä¾‹
model = MultiModalFusion(n_modalities=6, hidden_dim=768)
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

# è®­ç»ƒå¾ªç¯
for batch in dataloader:
    optimizer.zero_grad()
    
    # å‰å‘ä¼ æ’­
    fused_features = model(batch)
    predictions = prediction_head(fused_features)
    
    # è®¡ç®—æŸå¤±
    loss = criterion(predictions, batch['labels'])
    
    # åå‘ä¼ æ’­
    loss.backward()
    optimizer.step()
""", language='python')
    
    with tab2:
        st.code("""
# config.yaml
model:
  name: "MultiModalFusion"
  n_modalities: 6
  hidden_dim: 768
  
encoders:
  mfbert:
    pretrained: "mfbert-base"
    freeze_layers: 8
    
  transformer:
    num_layers: 6
    num_heads: 8
    dim_feedforward: 2048
    
  bigru:
    input_size: 1024
    hidden_size: 384
    num_layers: 2
    bidirectional: true
    
  gcn:
    input_dim: 78
    hidden_dims: [256, 512, 768]
    num_layers: 3
    
  schnet:
    hidden_channels: 768
    num_filters: 128
    num_interactions: 6
    
  molformer:
    pretrained: "molformer-xl"
    max_length: 512

fusion:
  method: "adaptive_gating"
  use_cross_attention: true
  dropout: 0.3
  
training:
  batch_size: 32
  learning_rate: 1e-4
  weight_decay: 0.01
  warmup_steps: 1000
  max_epochs: 100
  
  # å¤šGPUè®­ç»ƒ
  distributed:
    enabled: true
    backend: "nccl"
    world_size: 4
""", language='yaml')
    
    with tab3:
        st.code("""
# train_multimodal.py
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
import wandb

def train_multimodal_fusion(config):
    '''å¤šæ¨¡æ€èåˆè®­ç»ƒè„šæœ¬'''
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ
    if config.distributed.enabled:
        dist.init_process_group(backend=config.distributed.backend)
        local_rank = dist.get_rank()
        torch.cuda.set_device(local_rank)
        device = torch.device(f'cuda:{local_rank}')
    else:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åˆå§‹åŒ–wandb
    wandb.init(project="multimodal-fusion", config=config)
    
    # åˆ›å»ºæ¨¡å‹
    model = MultiModalFusion(
        n_modalities=config.model.n_modalities,
        hidden_dim=config.model.hidden_dim
    ).to(device)
    
    if config.distributed.enabled:
        model = DDP(model, device_ids=[local_rank])
    
    # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.training.learning_rate,
        weight_decay=config.training.weight_decay
    )
    
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=config.training.warmup_steps,
        num_training_steps=len(train_loader) * config.training.max_epochs
    )
    
    # è®­ç»ƒå¾ªç¯
    best_val_loss = float('inf')
    
    for epoch in range(config.training.max_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        
        for batch_idx, batch in enumerate(train_loader):
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            batch = {k: v.to(device) for k, v in batch.items()}
            
            # å‰å‘ä¼ æ’­
            fused_features = model(batch)
            predictions = prediction_head(fused_features)
            loss = criterion(predictions, batch['labels'])
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            scheduler.step()
            
            train_loss += loss.item()
            
            # è®°å½•åˆ°wandb
            if batch_idx % 100 == 0:
                wandb.log({
                    'train_loss': loss.item(),
                    'learning_rate': scheduler.get_last_lr()[0]
                })
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        val_predictions = []
        val_labels = []
        
        with torch.no_grad():
            for batch in val_loader:
                batch = {k: v.to(device) for k, v in batch.items()}
                
                fused_features = model(batch)
                predictions = prediction_head(fused_features)
                loss = criterion(predictions, batch['labels'])
                
                val_loss += loss.item()
                val_predictions.extend(predictions.cpu().numpy())
                val_labels.extend(batch['labels'].cpu().numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        val_loss /= len(val_loader)
        val_r2 = r2_score(val_labels, val_predictions)
        val_rmse = np.sqrt(mean_squared_error(val_labels, val_predictions))
        
        # è®°å½•éªŒè¯æŒ‡æ ‡
        wandb.log({
            'val_loss': val_loss,
            'val_r2': val_r2,
            'val_rmse': val_rmse,
            'epoch': epoch
        })
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_loss,
                'val_r2': val_r2
            }, f'best_model_epoch_{epoch}.pt')
            
            wandb.save(f'best_model_epoch_{epoch}.pt')
        
        print(f'Epoch {epoch}: Train Loss={train_loss/len(train_loader):.4f}, '
              f'Val Loss={val_loss:.4f}, Val RÂ²={val_r2:.4f}')
    
    # æ¸…ç†
    if config.distributed.enabled:
        dist.destroy_process_group()
    
    wandb.finish()

if __name__ == '__main__':
    config = load_config('config.yaml')
    train_multimodal_fusion(config)
""", language='python')

# æœ€åæ·»åŠ æ€§èƒ½åŸºå‡†æµ‹è¯•
def show_performance_benchmarks():
    """å±•ç¤ºä¸åŒèåˆæ¶æ„çš„æ€§èƒ½åŸºå‡†"""
    st.markdown("### ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•")
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•æ•°æ®
    benchmarks = pd.DataFrame({
        'æ¶æ„': [
            'å•æ¨¡æ€-MFBERT', 'å•æ¨¡æ€-GCN', 'å•æ¨¡æ€-Transformer',
            'ä¸‰æ¨¡æ€-MMFDL', 'å››æ¨¡æ€-æ ‡å‡†', 'å…­æ¨¡æ€-æ‰©å±•', 'å…¨æ¨¡æ€-å®éªŒ'
        ],
        'Delaney RÂ²': [0.970, 0.920, 0.950, 0.960, 0.975, 0.982, 0.985],
        'Lipophilicity RÂ²': [0.820, 0.640, 0.650, 0.790, 0.865, 0.885, 0.890],
        'BACE RÂ²': [0.850, 0.590, 0.700, 0.820, 0.875, 0.890, 0.895],
        'å¹³å‡è®­ç»ƒæ—¶é—´(h)': [2.5, 1.5, 2.0, 4.5, 6.0, 9.0, 15.0],
        'GPUå†…å­˜(GB)': [8, 4, 6, 12, 16, 24, 32]
    })
    
    # æ€§èƒ½é›·è¾¾å›¾
    categories = ['Delaney', 'Lipophilicity', 'BACE', 'æ•ˆç‡', 'èµ„æº']
    
    fig = go.Figure()
    
    for idx, row in benchmarks.iterrows():
        # å½’ä¸€åŒ–æ•°æ®
        values = [
            row['Delaney RÂ²'],
            row['Lipophilicity RÂ²'],
            row['BACE RÂ²'],
            1 - row['å¹³å‡è®­ç»ƒæ—¶é—´(h)']/20,  # æ—¶é—´æ•ˆç‡
            1 - row['GPUå†…å­˜(GB)']/40  # èµ„æºæ•ˆç‡
        ]
        
        fig.add_trace(go.Scatterpolar(
            r=values,
            theta=categories,
            fill='toself',
            name=row['æ¶æ„']
        ))
    
    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, 1]
            )),
        title="å¤šæ¨¡æ€èåˆæ¶æ„ç»¼åˆæ€§èƒ½å¯¹æ¯”",
        showlegend=True
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # æ¨èæ–¹æ¡ˆ
    st.success("""
    ğŸ¯ **æ¨èæ–¹æ¡ˆæ€»ç»“**
    
    1. **ç”Ÿäº§ç¯å¢ƒ**: å››æ¨¡æ€æ ‡å‡†æ¶æ„
       - æ€§èƒ½ç¨³å®šï¼Œç»è¿‡å……åˆ†éªŒè¯
       - èµ„æºéœ€æ±‚é€‚ä¸­
    
    2. **ç ”ç©¶æ¢ç´¢**: å…­æ¨¡æ€æ‰©å±•æ¶æ„  
       - æ€§èƒ½æå‡æ˜æ˜¾
       - ä¿æŒè‰¯å¥½çš„è®­ç»ƒæ•ˆç‡
    
    3. **æé™æ€§èƒ½**: å…¨æ¨¡æ€å®éªŒæ¶æ„
       - æœ€é«˜é¢„æµ‹ç²¾åº¦
       - éœ€è¦å¤§é‡è®¡ç®—èµ„æº
    """)